{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch  # Add this import\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已讀取第1筆資料\n"
     ]
    }
   ],
   "source": [
    "i = input('請輸入要讀取的資料編號(1~17):')\n",
    "while i.isdigit() == False or int(i) < 1 or int(i) > 17:\n",
    "    i = input('輸入錯誤，請重新輸入:')\n",
    "    \n",
    "train_file_paths_single = f'36_TrainingData/L{i}_Train.csv'#讀取單一資料\n",
    "df_data = pd.read_csv(train_file_paths_single)\n",
    "print(f'已讀取第{i}筆資料')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model and data to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "aicup_train = pd.read_csv(f'36_TrainingData/L{i}_Train_analysised.csv')\n",
    "aicup_train.drop(columns=[\"DateTime\"], inplace=True)\n",
    "aicup_train_features = aicup_train.drop(columns=[\"Power(mW)\"])\n",
    "aicup_train_target = aicup_train[\"Power(mW)\"]\n",
    "torch.version.cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (81338, 5)\n",
      "Testing features shape: (20335, 5)\n",
      "Training target shape: (81338,)\n",
      "Testing target shape: (20335,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(aicup_train_features, aicup_train_target, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Testing target shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5000], Loss: 311016.3750\n",
      "Epoch [2/5000], Loss: 310977.6250\n",
      "Epoch [3/5000], Loss: 310941.3438\n",
      "Epoch [4/5000], Loss: 310904.5625\n",
      "Epoch [5/5000], Loss: 310866.5938\n",
      "Epoch [6/5000], Loss: 310827.3750\n",
      "Epoch [7/5000], Loss: 310789.5625\n",
      "Epoch [8/5000], Loss: 310754.0625\n",
      "Epoch [9/5000], Loss: 310719.0625\n",
      "Epoch [10/5000], Loss: 310683.8750\n",
      "Epoch [11/5000], Loss: 310646.8750\n",
      "Epoch [12/5000], Loss: 310610.3750\n",
      "Epoch [13/5000], Loss: 310574.6875\n",
      "Epoch [14/5000], Loss: 310540.0000\n",
      "Epoch [15/5000], Loss: 310505.9688\n",
      "Epoch [16/5000], Loss: 310472.0312\n",
      "Epoch [17/5000], Loss: 310438.4688\n",
      "Epoch [18/5000], Loss: 310404.0625\n",
      "Epoch [19/5000], Loss: 310369.4062\n",
      "Epoch [20/5000], Loss: 310335.5312\n",
      "Epoch [21/5000], Loss: 310301.6250\n",
      "Epoch [22/5000], Loss: 310267.5000\n",
      "Epoch [23/5000], Loss: 310233.2188\n",
      "Epoch [24/5000], Loss: 310198.9375\n",
      "Epoch [25/5000], Loss: 310164.5312\n",
      "Epoch [26/5000], Loss: 310130.3750\n",
      "Epoch [27/5000], Loss: 310098.6562\n",
      "Epoch [28/5000], Loss: 310059.9375\n",
      "Epoch [29/5000], Loss: 310024.6562\n",
      "Epoch [30/5000], Loss: 309988.5625\n",
      "Epoch [31/5000], Loss: 309952.3438\n",
      "Epoch [32/5000], Loss: 309917.9375\n",
      "Epoch [33/5000], Loss: 309879.7188\n",
      "Epoch [34/5000], Loss: 309844.9062\n",
      "Epoch [35/5000], Loss: 309810.1250\n",
      "Epoch [36/5000], Loss: 309776.3125\n",
      "Epoch [37/5000], Loss: 309741.1875\n",
      "Epoch [38/5000], Loss: 309706.0000\n",
      "Epoch [39/5000], Loss: 309670.1875\n",
      "Epoch [40/5000], Loss: 309634.2500\n",
      "Epoch [41/5000], Loss: 309599.0000\n",
      "Epoch [42/5000], Loss: 309562.7188\n",
      "Epoch [43/5000], Loss: 309527.0000\n",
      "Epoch [44/5000], Loss: 309491.4688\n",
      "Epoch [45/5000], Loss: 309456.0938\n",
      "Epoch [46/5000], Loss: 309420.4375\n",
      "Epoch [47/5000], Loss: 309385.3125\n",
      "Epoch [48/5000], Loss: 309349.7812\n",
      "Epoch [49/5000], Loss: 309314.0000\n",
      "Epoch [50/5000], Loss: 309277.0312\n",
      "Epoch [51/5000], Loss: 309241.0000\n",
      "Epoch [52/5000], Loss: 309204.7812\n",
      "Epoch [53/5000], Loss: 309170.3750\n",
      "Epoch [54/5000], Loss: 309132.0312\n",
      "Epoch [55/5000], Loss: 309096.4688\n",
      "Epoch [56/5000], Loss: 309061.0312\n",
      "Epoch [57/5000], Loss: 309024.8750\n",
      "Epoch [58/5000], Loss: 308988.2188\n",
      "Epoch [59/5000], Loss: 308975.5625\n",
      "Epoch [60/5000], Loss: 308915.0000\n",
      "Epoch [61/5000], Loss: 308878.2812\n",
      "Epoch [62/5000], Loss: 308841.3750\n",
      "Epoch [63/5000], Loss: 308803.2812\n",
      "Epoch [64/5000], Loss: 308766.2812\n",
      "Epoch [65/5000], Loss: 308728.3438\n",
      "Epoch [66/5000], Loss: 308690.3438\n",
      "Epoch [67/5000], Loss: 308651.2500\n",
      "Epoch [68/5000], Loss: 308615.9062\n",
      "Epoch [69/5000], Loss: 308577.3750\n",
      "Epoch [70/5000], Loss: 308541.7188\n",
      "Epoch [71/5000], Loss: 308502.9062\n",
      "Epoch [72/5000], Loss: 308466.3438\n",
      "Epoch [73/5000], Loss: 308430.5312\n",
      "Epoch [74/5000], Loss: 308395.0000\n",
      "Epoch [75/5000], Loss: 308358.5625\n",
      "Epoch [76/5000], Loss: 308322.7812\n",
      "Epoch [77/5000], Loss: 308287.7188\n",
      "Epoch [78/5000], Loss: 308252.8750\n",
      "Epoch [79/5000], Loss: 308216.7812\n",
      "Epoch [80/5000], Loss: 308181.2500\n",
      "Epoch [81/5000], Loss: 308150.4688\n",
      "Epoch [82/5000], Loss: 308114.9062\n",
      "Epoch [83/5000], Loss: 308077.2812\n",
      "Epoch [84/5000], Loss: 308043.4375\n",
      "Epoch [85/5000], Loss: 308012.6250\n",
      "Epoch [86/5000], Loss: 307977.0312\n",
      "Epoch [87/5000], Loss: 307944.3438\n",
      "Epoch [88/5000], Loss: 307911.7500\n",
      "Epoch [89/5000], Loss: 307879.8438\n",
      "Epoch [90/5000], Loss: 307845.2812\n",
      "Epoch [91/5000], Loss: 307811.2812\n",
      "Epoch [92/5000], Loss: 307780.5000\n",
      "Epoch [93/5000], Loss: 307742.3125\n",
      "Epoch [94/5000], Loss: 307705.2500\n",
      "Epoch [95/5000], Loss: 307671.0625\n",
      "Epoch [96/5000], Loss: 307637.2188\n",
      "Epoch [97/5000], Loss: 307603.7188\n",
      "Epoch [98/5000], Loss: 307569.5625\n",
      "Epoch [99/5000], Loss: 307535.8438\n",
      "Epoch [100/5000], Loss: 307501.7500\n",
      "Epoch [101/5000], Loss: 307467.6250\n",
      "Epoch [102/5000], Loss: 307435.5938\n",
      "Epoch [103/5000], Loss: 307403.2188\n",
      "Epoch [104/5000], Loss: 307371.1562\n",
      "Epoch [105/5000], Loss: 307339.4375\n",
      "Epoch [106/5000], Loss: 307307.1875\n",
      "Epoch [107/5000], Loss: 307274.7500\n",
      "Epoch [108/5000], Loss: 307276.0938\n",
      "Epoch [109/5000], Loss: 307212.2812\n",
      "Epoch [110/5000], Loss: 307178.6875\n",
      "Epoch [111/5000], Loss: 307146.8125\n",
      "Epoch [112/5000], Loss: 307114.6562\n",
      "Epoch [113/5000], Loss: 307083.6875\n",
      "Epoch [114/5000], Loss: 307052.3125\n",
      "Epoch [115/5000], Loss: 307021.6250\n",
      "Epoch [116/5000], Loss: 306992.8438\n",
      "Epoch [117/5000], Loss: 306960.3750\n",
      "Epoch [118/5000], Loss: 306930.1250\n",
      "Epoch [119/5000], Loss: 306899.7812\n",
      "Epoch [120/5000], Loss: 306869.3438\n",
      "Epoch [121/5000], Loss: 306839.3750\n",
      "Epoch [122/5000], Loss: 306808.4062\n",
      "Epoch [123/5000], Loss: 306777.4062\n",
      "Epoch [124/5000], Loss: 306752.3750\n",
      "Epoch [125/5000], Loss: 306715.3438\n",
      "Epoch [126/5000], Loss: 306740.0625\n",
      "Epoch [127/5000], Loss: 306656.2812\n",
      "Epoch [128/5000], Loss: 306624.9375\n",
      "Epoch [129/5000], Loss: 306595.8438\n",
      "Epoch [130/5000], Loss: 306567.0000\n",
      "Epoch [131/5000], Loss: 306537.5312\n",
      "Epoch [132/5000], Loss: 306507.8750\n",
      "Epoch [133/5000], Loss: 306477.6562\n",
      "Epoch [134/5000], Loss: 306447.5000\n",
      "Epoch [135/5000], Loss: 306417.4062\n",
      "Epoch [136/5000], Loss: 306386.5312\n",
      "Epoch [137/5000], Loss: 306364.5000\n",
      "Epoch [138/5000], Loss: 306327.4062\n",
      "Epoch [139/5000], Loss: 306297.3750\n",
      "Epoch [140/5000], Loss: 306267.7812\n",
      "Epoch [141/5000], Loss: 306237.5625\n",
      "Epoch [142/5000], Loss: 306210.2188\n",
      "Epoch [143/5000], Loss: 306178.7500\n",
      "Epoch [144/5000], Loss: 306150.3750\n",
      "Epoch [145/5000], Loss: 306120.4688\n",
      "Epoch [146/5000], Loss: 306089.2812\n",
      "Epoch [147/5000], Loss: 306059.6250\n",
      "Epoch [148/5000], Loss: 306028.3125\n",
      "Epoch [149/5000], Loss: 305999.3750\n",
      "Epoch [150/5000], Loss: 305970.5312\n",
      "Epoch [151/5000], Loss: 305941.6875\n",
      "Epoch [152/5000], Loss: 305912.8438\n",
      "Epoch [153/5000], Loss: 305885.6562\n",
      "Epoch [154/5000], Loss: 305855.3125\n",
      "Epoch [155/5000], Loss: 305826.5938\n",
      "Epoch [156/5000], Loss: 305797.9375\n",
      "Epoch [157/5000], Loss: 305769.2500\n",
      "Epoch [158/5000], Loss: 305740.8438\n",
      "Epoch [159/5000], Loss: 305712.2188\n",
      "Epoch [160/5000], Loss: 305683.7500\n",
      "Epoch [161/5000], Loss: 305656.2188\n",
      "Epoch [162/5000], Loss: 305626.7500\n",
      "Epoch [163/5000], Loss: 305598.2188\n",
      "Epoch [164/5000], Loss: 305602.8438\n",
      "Epoch [165/5000], Loss: 305541.3750\n",
      "Epoch [166/5000], Loss: 305512.9688\n",
      "Epoch [167/5000], Loss: 305484.5312\n",
      "Epoch [168/5000], Loss: 305456.3438\n",
      "Epoch [169/5000], Loss: 305427.5000\n",
      "Epoch [170/5000], Loss: 305532.9062\n",
      "Epoch [171/5000], Loss: 305372.0000\n",
      "Epoch [172/5000], Loss: 305343.3750\n",
      "Epoch [173/5000], Loss: 305315.2188\n",
      "Epoch [174/5000], Loss: 305288.7812\n",
      "Epoch [175/5000], Loss: 305258.8750\n",
      "Epoch [176/5000], Loss: 305230.7812\n",
      "Epoch [177/5000], Loss: 305207.6875\n",
      "Epoch [178/5000], Loss: 305174.3750\n",
      "Epoch [179/5000], Loss: 305158.5625\n",
      "Epoch [180/5000], Loss: 305118.5000\n",
      "Epoch [181/5000], Loss: 305090.5625\n",
      "Epoch [182/5000], Loss: 305062.6250\n",
      "Epoch [183/5000], Loss: 305034.5938\n",
      "Epoch [184/5000], Loss: 305006.6875\n",
      "Epoch [185/5000], Loss: 304978.7188\n",
      "Epoch [186/5000], Loss: 304950.7188\n",
      "Epoch [187/5000], Loss: 304922.7188\n",
      "Epoch [188/5000], Loss: 304894.6875\n",
      "Epoch [189/5000], Loss: 304866.5625\n",
      "Epoch [190/5000], Loss: 304838.5312\n",
      "Epoch [191/5000], Loss: 304810.4375\n",
      "Epoch [192/5000], Loss: 304782.2188\n",
      "Epoch [193/5000], Loss: 304781.1875\n",
      "Epoch [194/5000], Loss: 304725.5312\n",
      "Epoch [195/5000], Loss: 304696.8750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert the training data to PyTorch tensors and move to the same device as the model\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = predict_model(X_train_tensor.unsqueeze(1))  # Add an extra dimension\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the testing data to PyTorch tensors and move to the same device as the model\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "predict_model.eval()\n",
    "\n",
    "# Forward pass on the test set\n",
    "with torch.no_grad():\n",
    "    test_outputs = predict_model(X_test_tensor)\n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
